{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d9HjnVDmssa8"
   },
   "outputs": [],
   "source": [
    "# from os import chdir\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "# chdir(\"/content/drive/MyDrive/Eliott/files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "v-9SGvosr0_w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import json\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_train = 'Corona_NLP_train.csv'\n",
    "file_name_test = 'Corona_NLP_test.csv'\n",
    "X_train, y_train = pp.prepare_dataframe(file_name_train,lemmatising=False)\n",
    "X_test, y_test = pp.prepare_dataframe(file_name_test,lemmatising=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève : \n",
    "- Les URLS\n",
    "- Hashtags\n",
    "- Mentions\n",
    "- Mots réservés\n",
    "- Emojis et smileys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 35525 mots sans lemmatisation\n",
    "- 30794 avec lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "On commence par vectoriser les données textuelles sous forme de tfidf. On se retrouve avec une sorte de one-hot vector pour chaque mot présent dans le corpus  total de tweets. La valeur pour chaque mot est le nombre de fois que le mot apparait dans le tweet divisé par le nombre de fois dans tous les tweets (importance locale / fréquence totale). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 35525)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "On va augmenter syntétiquement les données pour équilibrer les classes et améliorer les performances globales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    18046\n",
      "-1    15398\n",
      " 0     7713\n",
      "Name: Sentiment_Number, dtype: int64\n",
      "-1    1633\n",
      " 1    1546\n",
      " 0     619\n",
      "Name: Sentiment_Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMY7FfWTr1AI"
   },
   "source": [
    "# Recherche d'Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = SGDClassifier()\n",
    "model_gb = GradientBoostingClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_params = {\n",
    "    \"SGD\" : { \"model\" : model_sgd,\n",
    "              \"params\" : {\n",
    "                    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                    'alpha': np.linspace(1e-7, 1e-4, 20),  \n",
    "                        }\n",
    "    },\n",
    "    \"GB\" : { \"model\" : model_gb,\n",
    "              \"params\" : {\n",
    "                    \"gb_clf__learning_rate\": [0.01, 0.1, 0.2],\n",
    "                    \"gb_clf__n_estimators\":[10,50,100,200,400,800]\n",
    "                        }\n",
    "    },\n",
    "    \"SGD\" : { \"model\" : model_gb,\n",
    "              \"params\" : {\n",
    "                    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                    'alpha': np.linspace(1e-7, 1e-4, 20),  \n",
    "                        }\n",
    "    }\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "En34oFshCc-r"
   },
   "outputs": [],
   "source": [
    "def grid_Search(model_name,X_search,Y_search):\n",
    "\n",
    "    model = models_and_params[model_name][\"model\"]\n",
    "    params = models_and_params[model_name][\"params\"]\n",
    "\n",
    "    grid_clf = GridSearchCV(model, params, verbose=1, scoring='accuracy' ,n_jobs=-1)\n",
    "    \n",
    "    grid_clf.fit(X_search, Y_search)\n",
    "\n",
    "    print(\"Best Score: \", grid_clf.best_score_)\n",
    "    print(\"Best Params: \", grid_clf.best_params_)\n",
    "\n",
    "    return grid_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Score:  0.8782468302592571\n",
      "Best Params:  {'alpha': 1.5873684210526315e-05, 'penalty': 'l1'}\n",
      "CPU times: user 2.05 s, sys: 2.86 s, total: 4.91 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_sgd = grid_Search(\"SGD\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Score:  0.8910936070869632\n",
      "Best Params:  {'alpha': 5.357894736842105e-06, 'penalty': 'l1'}\n",
      "CPU times: user 2.64 s, sys: 3.15 s, total: 5.79 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_sgd_smote = grid_Search(\"SGD\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-R5-LqYCc-s"
   },
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Js4osvIVCc-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_gb = grid_Search(\"GB\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_gb_smote = grid_Search(\"GB\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "L58wVHqXCc-s",
    "outputId": "3f0828a6-faf9-4251-ac38-783416ab50e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "CPU times: user 9min 29s, sys: 258 ms, total: 9min 29s\n",
      "Wall time: 46min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subset = X_train.shape[0]\n",
    "\n",
    "gb_best = gb_clf.fit(X_train[:subset], y_train[:subset])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "FPjgbNPZCc-s",
    "outputId": "77ceb4a5-b879-4f76-a949-9b0e0582648f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8194232669897714\n",
      "Best Params:  {'gb_clf__learning_rate': 0.2, 'gb_clf__n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", gb_clf.best_score_)\n",
    "print(\"Best Params: \", gb_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf8DtN5iCc-t"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P5bHoZ9pCc-t"
   },
   "outputs": [],
   "source": [
    "randForest_param = {\n",
    "    'rf_clf__n_estimators': [10, 100],   \n",
    "    'rf_clf__max_depth': [10 ,150,300,600],\n",
    "    'rf_clf__min_samples_leaf': [1, 2, 3],   \n",
    "    'rf_clf__min_samples_split': [4, 8, 16, 32],\n",
    "    'rf_clf__max_features': ['log2', 'sqrt'],\n",
    "    'rf_clf__criterion': ['gini', 'entropy'],\n",
    "    'rf_clf__warm_start': [True, False] \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "pipeline_random_forest = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('rf_clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "rf_clf = GridSearchCV(pipeline_random_forest, randForest_param, scoring='accuracy', verbose=1 ,n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "a8oiQPa1Cc-t",
    "outputId": "6d19aab7-4f19-4f68-c15e-e875728e885f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "CPU times: user 1min 34s, sys: 8.71 s, total: 1min 43s\n",
      "Wall time: 42min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subset = X_train.shape[0]\n",
    "\n",
    "rf_best = rf_clf.fit(X_train[:subset], y_train[:subset])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "-iAg4caBCc-t",
    "outputId": "779675e0-c1bd-4b6f-a51a-3ce6c2b5e90e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.6925428813940141\n",
      "Best Params:  {'rf_clf__criterion': 'entropy', 'rf_clf__max_depth': 300, 'rf_clf__max_features': 'sqrt', 'rf_clf__min_samples_leaf': 2, 'rf_clf__min_samples_split': 32, 'rf_clf__n_estimators': 100, 'rf_clf__warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", rf_clf.best_score_)\n",
    "print(\"Best Params: \", rf_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hybcOGPZCc-t"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "1vYHWTnnCc-t"
   },
   "outputs": [],
   "source": [
    "LogisticRegression_param = {\n",
    "    'lr_clf__C': [100, 80, 40,20,10, 1.0],\n",
    "    'lr_clf__tol': np.linspace(1e-8,1e-4,15)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K2zGzzSbCc-u"
   },
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                       \n",
    "                     ('lr_clf', LogisticRegression(max_iter=1000)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "NKt3pLZBCc-u"
   },
   "outputs": [],
   "source": [
    "lr_clf = GridSearchCV(pipeline_lr, LogisticRegression_param,  scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "ah8rfObsCc-u",
    "outputId": "821f3a84-053c-4972-c10d-cf767dc40f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "CPU times: user 1min 55s, sys: 4min 27s, total: 6min 23s\n",
      "Wall time: 32min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_best = lr_clf.fit(X_train[:subset], y_train[:subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "WUqSxp-DCc-u",
    "outputId": "be7c2a1a-a3d9-451e-f042-d9cf9161eb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8230920602963577\n",
      "Best Params:  {'lr_clf__C': 10, 'lr_clf__tol': 1e-08}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", lr_clf.best_score_)\n",
    "print(\"Best Params: \", lr_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression_param = {\n",
    "    'C': [100, 80, 40,20,10, 1.0],\n",
    "    'tol': np.linspace(1e-8,1e-4,15)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsololr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_solo = GridSearchCV(modelsololr, LogisticRegression_param,verbose=1, scoring='accuracy' ,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 5min 43s, total: 7min 35s\n",
      "Wall time: 50min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_best_solo = lr_clf_solo.fit(X_new, yNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8546311385678151\n",
      "Best Params:  {'C': 20, 'tol': 1e-08}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", lr_clf_solo.best_score_)\n",
    "print(\"Best Params: \", lr_clf_solo.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCMXN9M8Cc-u"
   },
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NPGHPCruCc-v"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "nZ1wmcmnCc-v"
   },
   "outputs": [],
   "source": [
    "per_params = {\n",
    "  'per_clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "  'per_clf__alpha': np.linspace(1e-8, 1e-4, 20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i084u0cSCc-v"
   },
   "outputs": [],
   "source": [
    "pipeline_perceptron = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                       \n",
    "                     ('per_clf', Perceptron()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "4bXgV-wICc-v"
   },
   "outputs": [],
   "source": [
    "per_clf = GridSearchCV(pipeline_perceptron, per_params,  scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "-Y8KKKNqCc-v",
    "outputId": "9f7531d6-8757-4dfa-9aaf-65b27ec3ff87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "CPU times: user 7.8 s, sys: 3.47 s, total: 11.3 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "per_best = per_clf.fit(X_train[:subset], y_train[:subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "9dTgtWPwCc-v",
    "outputId": "850fa25d-7e8e-4673-8d08-aceccc172a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8128628567555942\n",
      "Best Params:  {'per_clf__alpha': 5.272631578947369e-06, 'per_clf__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", per_clf.best_score_)\n",
    "print(\"Best Params: \", per_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "\n",
    "La regression logistique n'est qu'un perceptron avec une sigmoid en fonction d'activation.\n",
    "On voit que la Regression Logistique a de meilleures performances à l'issue de la recherche d'hyperparamètres mais pas de loin. Par ailleurs le temps d'entrainement est considérablement plus élevé pour la regression logistique (du au calcul de l'exponentiel). Nous verrons par la suite quel modèle il est préférable de conserver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cy0mgWVCc-v"
   },
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9K-4Ufu3Cc-v"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l4IjTNIiCc-w"
   },
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "  'svc_clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "  'svc_clf__loss': ['hinge', 'squared_hinge'],\n",
    "  'svc_clf__dual' : [False,True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QsauLn7wCc-w"
   },
   "outputs": [],
   "source": [
    "pipeline_svc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                       \n",
    "                     ('svc_clf', LinearSVC(max_iter=10000)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1CW5MyCxCc-w"
   },
   "outputs": [],
   "source": [
    "svc_clf = GridSearchCV(pipeline_svc, svc_params,  scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "D-EHrYKACc-w",
    "outputId": "959d23ee-fd5c-4bb0-b465-e7bd5cdc0620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='hinge' is not supported, Parameters: penalty='elasticnet', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='squared_hinge' is not supported, Parameters: penalty='elasticnet', loss='squared_hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='hinge' is not supported, Parameters: penalty='elasticnet', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='squared_hinge' is not supported, Parameters: penalty='elasticnet', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.83094003 0.86894087        nan\n",
      " 0.82693114        nan        nan 0.83091573        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.45 s, sys: 812 ms, total: 8.26 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc_best = svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aB2TUBujCc-w",
    "outputId": "5562d375-7e68-49c7-8109-4979babcddbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8689408679104181\n",
      "Best Params:  {'svc_clf__dual': False, 'svc_clf__loss': 'squared_hinge', 'svc_clf__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", svc_clf.best_score_)\n",
    "print(\"Best Params: \", svc_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svc_params = {\n",
    "  'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "  'loss': ['hinge', 'squared_hinge'],\n",
    "  'dual' : [False,True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsolosvc = LinearSVC(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf_solo = GridSearchCV(modelsolosvc, svc_params,verbose=1, scoring='accuracy' ,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='hinge' is not supported, Parameters: penalty='elasticnet', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='squared_hinge' is not supported, Parameters: penalty='elasticnet', loss='squared_hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='hinge' is not supported, Parameters: penalty='elasticnet', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='elasticnet' and loss='squared_hinge' is not supported, Parameters: penalty='elasticnet', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.86081901 0.88398188        nan\n",
      " 0.85202663        nan        nan 0.86081901        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.14 s, sys: 173 ms, total: 4.31 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc_best_solo = svc_clf_solo.fit(X_new, yNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.883981883324771\n",
      "Best Params:  {'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \", svc_clf_solo.best_score_)\n",
    "print(\"Best Params: \", svc_clf_solo.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Hyperparameters in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json') as json_file:\n",
    "    dico = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['SGD'] = sgd_clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['RF'] = rf_clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['GB'] = gb_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['LR'] = lr_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['Perceptron'] = per_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['SVC'] = svc_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "okUYBXphCc-w"
   },
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charging the best parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ct7BBebdCc-w",
    "outputId": "a7c78c52-cb68-4e11-9490-fb43ef8b6189"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "data_sgd = data['SGD']\n",
    "data_rf = data['RF']\n",
    "data_gb = data['GB']\n",
    "data_lr = data['LR']\n",
    "data_per = data['Perceptron']\n",
    "data_svc = data['SVC']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on all training data and testing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model):\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_test = model.predict(X_test)\n",
    "    accuracy_train = accuracy_score(y_train,predictions_train )\n",
    "    accuracy_test = accuracy_score(y_test,predictions_test )\n",
    "    \n",
    "    print(f\"train_accuracy : {accuracy_train} \\ntest_accuracy : {accuracy_test}  \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "FozzlotYCc-w",
    "outputId": "82bdacbd-ae58-49a0-faaf-c3ab0308b5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=1.5873684210526315e-05, n_jobs=-1,\n",
       "                               penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sgd = pipeline_clf\n",
    "model_sgd.set_params(**data_sgd,clf__verbose=1,clf__n_jobs=-1) # clf__max_iter=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 405.71, NNZs: 5790, Bias: -1.648232, T: 41157, Avg. loss: 0.580203Norm: 359.38, NNZs: 5075, Bias: 1.019153, T: 41157, Avg. loss: 0.496203Norm: 418.90, NNZs: 5942, Bias: -1.727436, T: 41157, Avg. loss: 0.609151\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 367.78, NNZs: 3565, Bias: 0.959986, T: 82314, Avg. loss: 0.236487\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 424.82, NNZs: 4057, Bias: -1.548911, T: 82314, Avg. loss: 0.272227\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 412.36, NNZs: 3999, Bias: -1.368071, T: 82314, Avg. loss: 0.262424\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 372.77, NNZs: 3265, Bias: 0.949299, T: 123471, Avg. loss: 0.217699\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 428.20, NNZs: 3584, Bias: -1.361592, T: 123471, Avg. loss: 0.246139\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 416.00, NNZs: 3534, Bias: -1.291198, T: 123471, Avg. loss: 0.238317\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 376.23, NNZs: 3116, Bias: 1.026861, T: 164628, Avg. loss: 0.208458\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 418.65, NNZs: 3343, Bias: -1.238042, T: 164628, Avg. loss: 0.228444\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 430.60, NNZs: 3365, Bias: -1.305802, T: 164628, Avg. loss: 0.235403\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 378.84, NNZs: 3022, Bias: 0.965961, T: 205785, Avg. loss: 0.203303\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 420.67, NNZs: 3217, Bias: -1.277776, T: 205785, Avg. loss: 0.223564\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 432.48, NNZs: 3276, Bias: -1.304516, T: 205785, Avg. loss: 0.230230\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 380.94, NNZs: 2981, Bias: 1.004752, T: 246942, Avg. loss: 0.200485\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.39, NNZs: 3156, Bias: -1.233720, T: 246942, Avg. loss: 0.219780\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 434.08, NNZs: 3189, Bias: -1.286048, T: 246942, Avg. loss: 0.226792\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 382.71, NNZs: 2942, Bias: 1.015789, T: 288099, Avg. loss: 0.198282\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 423.86, NNZs: 3105, Bias: -1.247450, T: 288099, Avg. loss: 0.217959\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 435.50, NNZs: 3157, Bias: -1.251444, T: 288099, Avg. loss: 0.225014\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 384.18, NNZs: 2920, Bias: 1.026543, T: 329256, Avg. loss: 0.196534\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 425.14, NNZs: 3086, Bias: -1.181361, T: 329256, Avg. loss: 0.216144\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 436.69, NNZs: 3083, Bias: -1.250015, T: 329256, Avg. loss: 0.223116\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 385.51, NNZs: 2890, Bias: 1.014112, T: 370413, Avg. loss: 0.195393\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 426.27, NNZs: 3051, Bias: -1.234118, T: 370413, Avg. loss: 0.214620\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 437.75, NNZs: 3075, Bias: -1.233362, T: 370413, Avg. loss: 0.222116\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 386.69, NNZs: 2907, Bias: 1.016627, T: 411570, Avg. loss: 0.194137\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 427.30, NNZs: 3038, Bias: -1.195812, T: 411570, Avg. loss: 0.213981\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 438.73, NNZs: 3042, Bias: -1.204366, T: 411570, Avg. loss: 0.220809\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 387.75, NNZs: 2892, Bias: 0.965039, T: 452727, Avg. loss: 0.193028\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 428.23, NNZs: 3005, Bias: -1.164355, T: 452727, Avg. loss: 0.212951\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 439.62, NNZs: 3026, Bias: -1.240827, T: 452727, Avg. loss: 0.220205\n",
      "Total training time: 0.23 seconds.\n",
      "Norm: 388.71, NNZs: 2885, Bias: 1.021071, T: 493884, Avg. loss: 0.192543\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 12\n",
      "-- Epoch 13\n",
      "Norm: 389.59, NNZs: 2878, Bias: 1.008954, T: 535041, Avg. loss: 0.191547\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 440.45, NNZs: 3010, Bias: -1.210602, T: 493884, Avg. loss: 0.219595Norm: 429.08, NNZs: 2977, Bias: -1.186430, T: 493884, Avg. loss: 0.212334\n",
      "Total training time: 0.25 seconds.\n",
      "\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 13-- Epoch 13\n",
      "\n",
      "Norm: 390.40, NNZs: 2883, Bias: 1.020725, T: 576198, Avg. loss: 0.191154\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 429.87, NNZs: 2961, Bias: -1.188857, T: 535041, Avg. loss: 0.211420Norm: 441.21, NNZs: 2986, Bias: -1.213863, T: 535041, Avg. loss: 0.218887\n",
      "Total training time: 0.30 seconds.\n",
      "\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "-- Epoch 14\n",
      "Norm: 391.16, NNZs: 2886, Bias: 1.008671, T: 617355, Avg. loss: 0.190566\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 430.62, NNZs: 2948, Bias: -1.183203, T: 576198, Avg. loss: 0.210650\n",
      "Norm: 441.93, NNZs: 2964, Bias: -1.221125, T: 576198, Avg. loss: 0.218482Total training time: 0.31 seconds.\n",
      "\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "-- Epoch 15\n",
      "Norm: 391.85, NNZs: 2880, Bias: 1.012541, T: 658512, Avg. loss: 0.190058\n",
      "Total training time: 0.32 seconds.\n",
      "Convergence after 16 epochs took 0.32 seconds\n",
      "Norm: 431.30, NNZs: 2929, Bias: -1.205327, T: 617355, Avg. loss: 0.210372\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 442.59, NNZs: 2955, Bias: -1.214659, T: 617355, Avg. loss: 0.217845\n",
      "Total training time: 0.31 seconds.\n",
      "Convergence after 15 epochs took 0.31 seconds\n",
      "Norm: 431.96, NNZs: 2926, Bias: -1.179964, T: 658512, Avg. loss: 0.209857\n",
      "Total training time: 0.35 seconds.\n",
      "Convergence after 16 epochs took 0.35 seconds\n",
      "CPU times: user 1.65 s, sys: 1.02 s, total: 2.67 s\n",
      "Wall time: 1.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=1.5873684210526315e-05, n_jobs=-1,\n",
       "                               penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_sgd.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9031027528731443 \n",
      "test_accuracy : 0.8612427593470248  \n"
     ]
    }
   ],
   "source": [
    "testing(model_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPAWdBLOCc-x"
   },
   "outputs": [],
   "source": [
    "f1score = f1_score(y_test, ad_best_ss.predict(X_test_ss), average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'tfidf', 'svc_clf', 'tfidf__analyzer', 'tfidf__binary', 'tfidf__decode_error', 'tfidf__dtype', 'tfidf__encoding', 'tfidf__input', 'tfidf__lowercase', 'tfidf__max_df', 'tfidf__max_features', 'tfidf__min_df', 'tfidf__ngram_range', 'tfidf__norm', 'tfidf__preprocessor', 'tfidf__smooth_idf', 'tfidf__stop_words', 'tfidf__strip_accents', 'tfidf__sublinear_tf', 'tfidf__token_pattern', 'tfidf__tokenizer', 'tfidf__use_idf', 'tfidf__vocabulary', 'svc_clf__C', 'svc_clf__class_weight', 'svc_clf__dual', 'svc_clf__fit_intercept', 'svc_clf__intercept_scaling', 'svc_clf__loss', 'svc_clf__max_iter', 'svc_clf__multi_class', 'svc_clf__penalty', 'svc_clf__random_state', 'svc_clf__tol', 'svc_clf__verbose'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "lGouwXZ_Cc-x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('rf_clf',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=300,\n",
       "                                        max_features='sqrt', min_samples_leaf=2,\n",
       "                                        min_samples_split=32, n_jobs=-1,\n",
       "                                        verbose=1, warm_start=True))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = pipeline_random_forest\n",
    "model_rf.set_params(**data_rf,rf_clf__verbose=1,rf_clf__n_jobs=-1) # clf__max_iter=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "bdEu7AlyCc-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 669 ms, sys: 378 µs, total: 670 ms\n",
      "Wall time: 681 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliott/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:429: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('rf_clf',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=300,\n",
       "                                        max_features='sqrt', min_samples_leaf=2,\n",
       "                                        min_samples_split=32, n_jobs=-1,\n",
       "                                        verbose=1, warm_start=True))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "yWtUmtDsCc-x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.8653934932089317 \n",
      "test_accuracy : 0.6908899420747762  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "testing(model_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('gb_clf',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, n_estimators=800,\n",
       "                                            verbose=1))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gb = pipeline_gradient_boosting\n",
    "model_gb.set_params(**data_gb,gb_clf__verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 33s, sys: 9.32 ms, total: 9min 33s\n",
      "Wall time: 9min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('gb_clf',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2,\n",
       "                                            n_estimators=800))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9136963335520082 \n",
      "test_accuracy : 0.8156924697209057  \n"
     ]
    }
   ],
   "source": [
    "testing(model_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('lr_clf',\n",
       "                 LogisticRegression(C=10, max_iter=1000, n_jobs=-1, tol=1e-08,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = pipeline_lr\n",
    "model_lr.set_params(**data_lr,lr_clf__verbose=1,lr_clf__n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "CPU times: user 655 ms, sys: 293 ms, total: 949 ms\n",
      "Wall time: 14.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('lr_clf',\n",
       "                 LogisticRegression(C=10, max_iter=1000, n_jobs=-1, tol=1e-08,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.959277887115193 \n",
      "test_accuracy : 0.8230647709320695  \n"
     ]
    }
   ],
   "source": [
    "testing(model_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('per_clf',\n",
       "                 Perceptron(alpha=5.272631578947369e-06, n_jobs=-1,\n",
       "                            penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_per = pipeline_perceptron\n",
    "model_per.set_params(**data_per,per_clf__verbose=1,per_clf__n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 10 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.22 seconds.\n",
      "Convergence after 11 epochs took 0.22 seconds\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.28 seconds.\n",
      "Convergence after 15 epochs took 0.28 seconds\n",
      "CPU times: user 1.38 s, sys: 1.14 s, total: 2.52 s\n",
      "Wall time: 944 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('per_clf',\n",
       "                 Perceptron(alpha=5.272631578947369e-06, n_jobs=-1,\n",
       "                            penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_per.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.8576669825303107 \n",
      "test_accuracy : 0.7814639283833597  \n"
     ]
    }
   ],
   "source": [
    "testing(model_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('svc_clf',\n",
       "                 LinearSVC(dual=False, max_iter=10000, penalty='l1',\n",
       "                           verbose=1))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc = pipeline_svc\n",
    "model_svc.set_params(**data_svc,svc_clf__verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]...........*..........*....*\n",
      "optimization finished, #iter = 257\n",
      "Objective value = 13387.596363\n",
      "#nonzeros/#features = 4675/30795\n",
      ".............*...........*...*\n",
      "optimization finished, #iter = 271\n",
      "Objective value = 12866.620558\n",
      "#nonzeros/#features = 4813/30795\n",
      "...........*..........*...*.\n",
      "optimization finished, #iter = 250\n",
      "Objective value = 13900.984310\n",
      "CPU times: user 6.94 s, sys: 13.1 ms, total: 6.95 s\n",
      "Wall time: 6.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('svc_clf',\n",
       "                 LinearSVC(dual=False, max_iter=10000, penalty='l1',\n",
       "                           verbose=1))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nonzeros/#features = 4850/30795\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9274728478752096 \n",
      "test_accuracy : 0.8496577145866245  \n"
     ]
    }
   ],
   "source": [
    "testing(model_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aSjG01FhCc-x"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "86JLNLKor1AL"
   },
   "outputs": [],
   "source": [
    "clf1 = model_sgd\n",
    "\n",
    "clf2 = model_rf\n",
    "\n",
    "clf3 = model_gb\n",
    "\n",
    "clf4 = model_lr\n",
    "\n",
    "clf5 = model_per\n",
    "\n",
    "clf6 = model_svc\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(\n",
    "     estimators=[('sgd', clf1), ('rf', clf2), ('gb', clf3), ('lr', clf4), ('per', clf5), ('svc', clf6)],\n",
    "     voting='hard')\n",
    "\n",
    "eclf2 = VotingClassifier(\n",
    "     estimators=[('sgd', clf1), ('lr', clf4), ('per', clf5), ('svc', clf6)],\n",
    "     voting='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 360.88, NNZs: 5102, Bias: 0.947182, T: 41157, Avg. loss: 0.486335\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 414.01, NNZs: 5851, Bias: -1.909098, T: 41157, Avg. loss: 0.605988\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 406.08, NNZs: 5633, Bias: -1.600516, T: 41157, Avg. loss: 0.579718\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 420.66, NNZs: 4001, Bias: -1.523806, T: 82314, Avg. loss: 0.268741\n",
      "Total training time: 0.05 seconds.-- Epoch 2\n",
      "Norm: 369.55, NNZs: 3544, Bias: 1.069173, T: 82314, Avg. loss: 0.234354\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 412.70, NNZs: 3927, Bias: -1.462631, T: 82314, Avg. loss: 0.262655\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 374.69, NNZs: 3254, Bias: 1.027108, T: 123471, Avg. loss: 0.214995\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 416.32, NNZs: 3547, Bias: -1.238832, T: 123471, Avg. loss: 0.238725\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 424.34, NNZs: 3624, Bias: -1.369941, T: 123471, Avg. loss: 0.243591\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 378.15, NNZs: 3109, Bias: 0.973131, T: 164628, Avg. loss: 0.207548\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 418.92, NNZs: 3333, Bias: -1.232200, T: 164628, Avg. loss: 0.228852\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 427.06, NNZs: 3418, Bias: -1.233322, T: 164628, Avg. loss: 0.234303\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 380.72, NNZs: 3029, Bias: 1.001786, T: 205785, Avg. loss: 0.202612\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 420.90, NNZs: 3186, Bias: -1.265533, T: 205785, Avg. loss: 0.223468\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 429.12, NNZs: 3284, Bias: -1.283154, T: 205785, Avg. loss: 0.228544\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 382.85, NNZs: 3000, Bias: 1.015469, T: 246942, Avg. loss: 0.199822\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.59, NNZs: 3110, Bias: -1.220478, T: 246942, Avg. loss: 0.220169\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 384.56, NNZs: 2974, Bias: 1.000921, T: 288099, Avg. loss: 0.197536\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 430.80, NNZs: 3210, Bias: -1.325734, T: 246942, Avg. loss: 0.225788\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 423.96, NNZs: 3046, Bias: -1.219762, T: 288099, Avg. loss: 0.217921\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 386.04, NNZs: 2962, Bias: 0.996184, T: 329256, Avg. loss: 0.195868\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 432.26, NNZs: 3142, Bias: -1.235629, T: 288099, Avg. loss: 0.223635\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 425.16, NNZs: 2991, Bias: -1.190760, T: 329256, Avg. loss: 0.216155\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 387.31, NNZs: 2953, Bias: 1.037875, T: 370413, Avg. loss: 0.194615\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 433.51, NNZs: 3103, Bias: -1.247245, T: 329256, Avg. loss: 0.222504\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 426.27, NNZs: 2960, Bias: -1.176976, T: 370413, Avg. loss: 0.215096\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 388.49, NNZs: 2934, Bias: 1.023350, T: 411570, Avg. loss: 0.193641\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 434.63, NNZs: 3079, Bias: -1.219030, T: 370413, Avg. loss: 0.221089\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 389.55, NNZs: 2945, Bias: 0.990808, T: 452727, Avg. loss: 0.192532\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 427.25, NNZs: 2940, Bias: -1.210376, T: 411570, Avg. loss: 0.214187\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 435.62, NNZs: 3041, Bias: -1.237477, T: 411570, Avg. loss: 0.219870\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 390.49, NNZs: 2918, Bias: 1.013324, T: 493884, Avg. loss: 0.191605\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 428.15, NNZs: 2933, Bias: -1.172813, T: 452727, Avg. loss: 0.212939\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 436.56, NNZs: 3039, Bias: -1.202731, T: 452727, Avg. loss: 0.219149\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 391.34, NNZs: 2898, Bias: 1.014393, T: 535041, Avg. loss: 0.191014\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 428.97, NNZs: 2896, Bias: -1.169951, T: 493884, Avg. loss: 0.212359\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 437.41, NNZs: 3018, Bias: -1.206500, T: 493884, Avg. loss: 0.218719\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 392.14, NNZs: 2893, Bias: 1.015973, T: 576198, Avg. loss: 0.190428\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 429.72, NNZs: 2897, Bias: -1.162087, T: 535041, Avg. loss: 0.211676\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 438.18, NNZs: 3009, Bias: -1.217034, T: 535041, Avg. loss: 0.218236\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 392.88, NNZs: 2902, Bias: 1.029818, T: 617355, Avg. loss: 0.189861\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 430.44, NNZs: 2876, Bias: -1.163043, T: 576198, Avg. loss: 0.211078\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 438.90, NNZs: 2991, Bias: -1.219193, T: 576198, Avg. loss: 0.217487\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 393.58, NNZs: 2887, Bias: 1.015620, T: 658512, Avg. loss: 0.189353\n",
      "Total training time: 0.30 seconds.\n",
      "Convergence after 16 epochs took 0.30 seconds\n",
      "Norm: 431.11, NNZs: 2870, Bias: -1.175340, T: 617355, Avg. loss: 0.210728\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 439.57, NNZs: 2982, Bias: -1.196030, T: 617355, Avg. loss: 0.217397\n",
      "Total training time: 0.30 seconds.\n",
      "Convergence after 15 epochs took 0.30 seconds\n",
      "Norm: 431.74, NNZs: 2858, Bias: -1.171222, T: 658512, Avg. loss: 0.210209\n",
      "Total training time: 0.31 seconds.\n",
      "Convergence after 16 epochs took 0.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0120            9.95m\n",
      "         2           0.9888            9.90m\n",
      "         3           0.9711            9.89m\n",
      "         4           0.9563            9.90m\n",
      "         5           0.9428            9.90m\n",
      "         6           0.9318            9.88m\n",
      "         7           0.9217            9.86m\n",
      "         8           0.9126            9.85m\n",
      "         9           0.9040            9.83m\n",
      "        10           0.8965            9.82m\n",
      "        20           0.8405            9.62m\n",
      "        30           0.8040            9.44m\n",
      "        40           0.7767            9.29m\n",
      "        50           0.7543            9.14m\n",
      "        60           0.7351            9.00m\n",
      "        70           0.7185            8.87m\n",
      "        80           0.7038            8.74m\n",
      "        90           0.6908            8.62m\n",
      "       100           0.6784            8.50m\n",
      "       200           0.5897            7.40m\n",
      "       300           0.5325            6.22m\n",
      "       400           0.4876            4.94m\n",
      "       500           0.4530            3.73m\n",
      "       600           0.4249            2.51m\n",
      "       700           0.4016            1.25m\n",
      "       800           0.3815            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 10 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.21 seconds.\n",
      "Convergence after 11 epochs took 0.21 seconds\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.26 seconds.\n",
      "Convergence after 15 epochs took 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]...........*..........*....**\n",
      "optimization finished, #iter = 254\n",
      "Objective value = 13387.596365\n",
      "#nonzeros/#features = 4668/30795\n",
      ".............*..........*...*\n",
      "optimization finished, #iter = 266\n",
      "Objective value = 12866.624502\n",
      "#nonzeros/#features = 4818/30795\n",
      "...........*..........*....*\n",
      "optimization finished, #iter = 251\n",
      "Objective value = 13900.999692\n",
      "#nonzeros/#features = 4853/30795\n",
      "CPU times: user 10min 8s, sys: 2.38 s, total: 10min 10s\n",
      "Wall time: 10min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('clf',\n",
       "                                               SGDClassifier(alpha=1.5873684210526315e-05,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             penalty='l1',\n",
       "                                                             verbose=1))])),\n",
       "                             ('rf',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('rf_clf',\n",
       "                                               RandomForestClassifier(criterion='entropy',\n",
       "                                                                      max_depth=300,\n",
       "                                                                      max_features='sqrt',\n",
       "                                                                      min_samples_leaf=2,\n",
       "                                                                      min_samples_split=32,\n",
       "                                                                      n_job...\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('lr_clf',\n",
       "                                               LogisticRegression(C=10,\n",
       "                                                                  max_iter=1000,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  tol=1e-08,\n",
       "                                                                  verbose=1))])),\n",
       "                             ('per',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('per_clf',\n",
       "                                               Perceptron(alpha=5.272631578947369e-06,\n",
       "                                                          n_jobs=-1,\n",
       "                                                          penalty='l1',\n",
       "                                                          verbose=1))])),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('svc_clf',\n",
       "                                               LinearSVC(dual=False,\n",
       "                                                         max_iter=10000,\n",
       "                                                         penalty='l1',\n",
       "                                                         verbose=1))]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eclf1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.937944942537114 \n",
      "test_accuracy : 0.8499210110584519  \n"
     ]
    }
   ],
   "source": [
    "testing(eclf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 357.60, NNZs: 4998, Bias: 0.891696, T: 41157, Avg. loss: 0.484172\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 366.71, NNZs: 3474, Bias: 0.954372, T: 82314, Avg. loss: 0.235496\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 412.60, NNZs: 5878, Bias: -1.926124, T: 41157, Avg. loss: 0.598335\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 404.48, NNZs: 5595, Bias: -1.558111, T: 41157, Avg. loss: 0.571228\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 371.66, NNZs: 3172, Bias: 1.002502, T: 123471, Avg. loss: 0.215340\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 375.23, NNZs: 3087, Bias: 0.988975, T: 164628, Avg. loss: 0.207595\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 419.40, NNZs: 4059, Bias: -1.519952, T: 82314, Avg. loss: 0.269418\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 377.89, NNZs: 3030, Bias: 1.001752, T: 205785, Avg. loss: 0.202337\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 411.28, NNZs: 3882, Bias: -1.333694, T: 82314, Avg. loss: 0.263669\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 379.95, NNZs: 2992, Bias: 0.999959, T: 246942, Avg. loss: 0.199732\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.83, NNZs: 3610, Bias: -1.359196, T: 123471, Avg. loss: 0.246532\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 414.99, NNZs: 3507, Bias: -1.275707, T: 123471, Avg. loss: 0.239624\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 381.78, NNZs: 2972, Bias: 0.993897, T: 288099, Avg. loss: 0.197770\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 417.59, NNZs: 3304, Bias: -1.266331, T: 164628, Avg. loss: 0.229658\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 425.45, NNZs: 3418, Bias: -1.280198, T: 164628, Avg. loss: 0.235829\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 383.28, NNZs: 2943, Bias: 1.020195, T: 329256, Avg. loss: 0.195586\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 419.64, NNZs: 3183, Bias: -1.222767, T: 205785, Avg. loss: 0.224534\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 427.45, NNZs: 3300, Bias: -1.283721, T: 205785, Avg. loss: 0.231521\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 384.60, NNZs: 2918, Bias: 1.007534, T: 370413, Avg. loss: 0.194475\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 421.28, NNZs: 3135, Bias: -1.235039, T: 246942, Avg. loss: 0.220891\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 429.07, NNZs: 3226, Bias: -1.272840, T: 246942, Avg. loss: 0.228047\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.74, NNZs: 3091, Bias: -1.199238, T: 288099, Avg. loss: 0.218625\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 385.78, NNZs: 2897, Bias: 1.015107, T: 411570, Avg. loss: 0.193198\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 430.47, NNZs: 3151, Bias: -1.222827, T: 288099, Avg. loss: 0.225689\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 386.83, NNZs: 2881, Bias: 1.001824, T: 452727, Avg. loss: 0.192620Norm: 423.96, NNZs: 3052, Bias: -1.212330, T: 329256, Avg. loss: 0.216954\n",
      "Total training time: 0.18 seconds.\n",
      "\n",
      "-- Epoch 12Total training time: 0.18 seconds.\n",
      "\n",
      "-- Epoch 9Norm: 431.69, NNZs: 3089, Bias: -1.253840, T: 329256, Avg. loss: 0.224195\n",
      "\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 387.78, NNZs: 2882, Bias: 1.046557, T: 493884, Avg. loss: 0.191802\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 425.07, NNZs: 3021, Bias: -1.174385, T: 370413, Avg. loss: 0.215612\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 432.77, NNZs: 3044, Bias: -1.235774, T: 370413, Avg. loss: 0.222932\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 388.67, NNZs: 2878, Bias: 0.992886, T: 535041, Avg. loss: 0.191133\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 433.76, NNZs: 3039, Bias: -1.224990, T: 411570, Avg. loss: 0.221752\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 426.05, NNZs: 2980, Bias: -1.186296, T: 411570, Avg. loss: 0.214361\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 389.47, NNZs: 2867, Bias: 1.018017, T: 576198, Avg. loss: 0.190395\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 434.66, NNZs: 3002, Bias: -1.229990, T: 452727, Avg. loss: 0.220669\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 426.95, NNZs: 2966, Bias: -1.193738, T: 452727, Avg. loss: 0.213601\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 390.24, NNZs: 2861, Bias: 1.021295, T: 617355, Avg. loss: 0.189997\n",
      "Total training time: 0.25 seconds.\n",
      "Convergence after 15 epochs took 0.25 seconds\n",
      "Norm: 435.48, NNZs: 2985, Bias: -1.217901, T: 493884, Avg. loss: 0.220406\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 427.78, NNZs: 2963, Bias: -1.207875, T: 493884, Avg. loss: 0.212646\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 436.24, NNZs: 2967, Bias: -1.215438, T: 535041, Avg. loss: 0.219369\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 428.54, NNZs: 2954, Bias: -1.188396, T: 535041, Avg. loss: 0.212085\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 436.96, NNZs: 2944, Bias: -1.219945, T: 576198, Avg. loss: 0.219144\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 429.25, NNZs: 2954, Bias: -1.199237, T: 576198, Avg. loss: 0.211706\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 437.64, NNZs: 2950, Bias: -1.203350, T: 617355, Avg. loss: 0.218462\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 429.91, NNZs: 2951, Bias: -1.186284, T: 617355, Avg. loss: 0.211087\n",
      "Total training time: 0.31 seconds.\n",
      "Convergence after 15 epochs took 0.31 seconds\n",
      "Norm: 438.28, NNZs: 2938, Bias: -1.170180, T: 658512, Avg. loss: 0.218251\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 438.86, NNZs: 2926, Bias: -1.192893, T: 699669, Avg. loss: 0.217804\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 439.43, NNZs: 2920, Bias: -1.198078, T: 740826, Avg. loss: 0.217458\n",
      "Total training time: 0.36 seconds.\n",
      "Convergence after 18 epochs took 0.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.16 seconds.\n",
      "Convergence after 10 epochs took 0.16 seconds\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 15 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.18 seconds.\n",
      "Convergence after 11 epochs took 0.18 seconds\n",
      "[LibLinear]...........*..........*....*\n",
      "optimization finished, #iter = 251\n",
      "Objective value = 13387.596367\n",
      "#nonzeros/#features = 4672/30795\n",
      "............*...........*....*\n",
      "optimization finished, #iter = 272\n",
      "Objective value = 12866.624466\n",
      "#nonzeros/#features = 4817/30795\n",
      "...........*..........*...*.\n",
      "optimization finished, #iter = 250\n",
      "Objective value = 13900.986476\n",
      "#nonzeros/#features = 4854/30795\n",
      "CPU times: user 9.47 s, sys: 1.77 s, total: 11.2 s\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('clf',\n",
       "                                               SGDClassifier(alpha=1.5873684210526315e-05,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             penalty='l1',\n",
       "                                                             verbose=1))])),\n",
       "                             ('lr',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('lr_clf',\n",
       "                                               LogisticRegression(C=10,\n",
       "                                                                  max_iter=1000,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  tol=1e-08,\n",
       "                                                                  verbose=1))])),\n",
       "                             ('per',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('per_clf',\n",
       "                                               Perceptron(alpha=5.272631578947369e-06,\n",
       "                                                          n_jobs=-1,\n",
       "                                                          penalty='l1',\n",
       "                                                          verbose=1))])),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('svc_clf',\n",
       "                                               LinearSVC(dual=False,\n",
       "                                                         max_iter=10000,\n",
       "                                                         penalty='l1',\n",
       "                                                         verbose=1))]))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eclf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9355881138081007 \n",
      "test_accuracy : 0.8522906793048973  \n"
     ]
    }
   ],
   "source": [
    "testing(eclf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_perso = [\"I think covid is the most horrible threat we ever faced\",\n",
    "                \"I love covid, thanks to it I can see my family much more often and I don't have to comute as much\",\n",
    "                \"I would love to come to your birthday party, but I got covid, I have to stay confined\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_out = eclf2.predict(tests_perso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main_notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "822dd2d5dd360abba25557036e653898358cd5c8dbb7b3b755070434497e6489"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
