{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d9HjnVDmssa8"
   },
   "outputs": [],
   "source": [
    "# from os import chdir\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "# chdir(\"/content/drive/MyDrive/Eliott/files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-9SGvosr0_w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 21:27:18.780100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-29 21:27:18.780170: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import json\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_train = 'Corona_NLP_train.csv'\n",
    "file_name_test = 'Corona_NLP_test.csv'\n",
    "X_train, y_train = pp.prepare_dataframe(file_name_train,lemmatising=False)\n",
    "X_test, y_test = pp.prepare_dataframe(file_name_test,lemmatising=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève : \n",
    "- Les URLS\n",
    "- Hashtags\n",
    "- Mentions\n",
    "- Mots réservés\n",
    "- Emojis et smileys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 35525 mots sans lemmatisation\n",
    "- 30794 avec lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "On commence par vectoriser les données textuelles sous forme de tfidf. On se retrouve avec une sorte de one-hot vector pour chaque mot présent dans le corpus  total de tweets. La valeur pour chaque mot est le nombre de fois que le mot apparait dans le tweet divisé par le nombre de fois dans tous les tweets (importance locale / fréquence totale). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 35525)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit(X_train)\n",
    "X_vect = vectorizer.transform(X_train)\n",
    "X_vect_test = vectorizer.transform(X_test)\n",
    "\n",
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "On va augmenter syntétiquement les données pour équilibrer les classes et améliorer les performances globales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    18046\n",
      "-1    15398\n",
      " 0     7713\n",
      "Name: Sentiment_Number, dtype: int64\n",
      "-1    1633\n",
      " 1    1546\n",
      " 0     619\n",
      "Name: Sentiment_Number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMY7FfWTr1AI"
   },
   "source": [
    "# Recherche d'Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = SGDClassifier()\n",
    "model_gb = GradientBoostingClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "model_lr = LogisticRegression(max_iter=1000)\n",
    "model_per = Perceptron()\n",
    "model_svc = LinearSVC(max_iter=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_params = {\n",
    "    \"SGD\" : { \"model\" : model_sgd,\n",
    "              \"params\" : {\n",
    "                    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                    'alpha': np.linspace(1e-7, 1e-4, 20),  \n",
    "                        }\n",
    "    },\n",
    "    \"GB\" : { \"model\" : model_gb,\n",
    "              \"params\" : {\n",
    "                    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "                    \"n_estimators\":[10,50,100,200,400,800]\n",
    "                        }\n",
    "    },\n",
    "    \"RF\" : { \"model\" : model_rf,\n",
    "              \"params\" : {\n",
    "                    'n_estimators': [10, 100],   \n",
    "                    'max_depth': [10 ,150,300,600],\n",
    "                    'min_samples_leaf': [1, 2, 3],   \n",
    "                    'min_samples_split': [4, 8, 16, 32],\n",
    "                    'max_features': ['log2', 'sqrt'],\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                    'warm_start': [True, False] \n",
    "                        }\n",
    "    },\n",
    "    \"LR\" : { \"model\" : model_lr,\n",
    "              \"params\" : {\n",
    "                    'C': [100, 80, 40,20,10, 1.0],\n",
    "                    'tol': np.linspace(1e-8,1e-4,15)\n",
    "                        }\n",
    "    },\n",
    "    \"PER\" : { \"model\" : model_per,\n",
    "              \"params\" : {\n",
    "                    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                    'alpha': np.linspace(1e-8, 1e-4, 20),\n",
    "                        }\n",
    "    },\n",
    "    \"SVC\" : { \"model\" : model_svc,\n",
    "              \"params\" : {\n",
    "                    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                    'loss': ['hinge', 'squared_hinge'],\n",
    "                    'dual' : [False,True]\n",
    "                        }\n",
    "    }\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "En34oFshCc-r"
   },
   "outputs": [],
   "source": [
    "def grid_Search(model_name,X_search,Y_search):\n",
    "\n",
    "    model = models_and_params[model_name][\"model\"]\n",
    "    parameters = models_and_params[model_name][\"params\"]\n",
    "\n",
    "    grid_clf = GridSearchCV(model, parameters, verbose=1, scoring='accuracy' ,n_jobs=-1)\n",
    "    \n",
    "    grid_clf.fit(X_search, Y_search)\n",
    "\n",
    "    print(\"Best Score: \", grid_clf.best_score_)\n",
    "    print(\"Best Params: \", grid_clf.best_params_)\n",
    "\n",
    "    return grid_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Hyperparameters in JSON\n",
    "\n",
    "We will first load old weights and updates only if needed. Then we can use the model with best params in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json') as json_file:\n",
    "    dico = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Score:  0.8782469129068223\n",
      "Best Params:  {'alpha': 1.5873684210526315e-05, 'penalty': 'l1'}\n",
      "CPU times: user 2.13 s, sys: 3.31 s, total: 5.44 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_sgd = grid_Search(\"SGD\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Score:  0.8910565975844229\n",
      "Best Params:  {'alpha': 5.357894736842105e-06, 'penalty': 'l1'}\n",
      "CPU times: user 3.06 s, sys: 3.1 s, total: 6.15 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_sgd_smote = grid_Search(\"SGD\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['SGD'] = grid_sgd_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-R5-LqYCc-s"
   },
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Js4osvIVCc-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb Cell 15'\u001b[0m in \u001b[0;36mgrid_Search\u001b[0;34m(model_name, X_search, Y_search)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb#ch0000013?line=3'>4</a>\u001b[0m params \u001b[39m=\u001b[39m models_and_params[model_name][\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb#ch0000013?line=5'>6</a>\u001b[0m grid_clf \u001b[39m=\u001b[39m GridSearchCV(model, params, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m ,n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb#ch0000013?line=7'>8</a>\u001b[0m grid_clf\u001b[39m.\u001b[39;49mfit(X_search, Y_search)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb#ch0000013?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Score: \u001b[39m\u001b[39m\"\u001b[39m, grid_clf\u001b[39m.\u001b[39mbest_score_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eliott/pCloudDrive/ELIOTTDOC/3A_ENSSAT/NotesCours/Hiver/TA/Projet_TA/Projet_TA/main_notebook.ipynb#ch0000013?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Params: \u001b[39m\u001b[39m\"\u001b[39m, grid_clf\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=884'>885</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=885'>886</a>\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=886'>887</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=888'>889</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=890'>891</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=892'>893</a>\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=893'>894</a>\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=894'>895</a>\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=1389'>1390</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=1390'>1391</a>\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=1391'>1392</a>\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=829'>830</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=830'>831</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=831'>832</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=832'>833</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=833'>834</a>\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=834'>835</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=835'>836</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=837'>838</a>\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=838'>839</a>\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=839'>840</a>\u001b[0m         clone(base_estimator),\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=840'>841</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=841'>842</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=842'>843</a>\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=843'>844</a>\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=844'>845</a>\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=845'>846</a>\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=846'>847</a>\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=847'>848</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=848'>849</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=849'>850</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=850'>851</a>\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=851'>852</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=852'>853</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=854'>855</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=855'>856</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=856'>857</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=857'>858</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=858'>859</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib64/python3.10/site-packages/sklearn/model_selection/_search.py?line=859'>860</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=1052'>1053</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=1054'>1055</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=1055'>1056</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=1056'>1057</a>\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=1057'>1058</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=932'>933</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=933'>934</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=934'>935</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=935'>936</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/parallel.py?line=936'>937</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=538'>539</a>\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=539'>540</a>\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=540'>541</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=541'>542</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=542'>543</a>\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/eliott/envs/RNenv/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=543'>544</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/concurrent/futures/_base.py?line=437'>438</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/concurrent/futures/_base.py?line=438'>439</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> <a href='file:///usr/lib64/python3.10/concurrent/futures/_base.py?line=440'>441</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/concurrent/futures/_base.py?line=442'>443</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/concurrent/futures/_base.py?line=443'>444</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib64/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/threading.py?line=317'>318</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/threading.py?line=318'>319</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib64/python3.10/threading.py?line=319'>320</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/threading.py?line=320'>321</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib64/python3.10/threading.py?line=321'>322</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_gb = grid_Search(\"GB\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_gb_smote = grid_Search(\"GB\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['GB'] = grid_gb_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf8DtN5iCc-t"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P5bHoZ9pCc-t"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_rf = grid_Search(\"RF\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_rf_smote = grid_Search(\"RF\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['RF'] = grid_rf_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hybcOGPZCc-t"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "NKt3pLZBCc-u"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_lr = grid_Search(\"LR\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "ah8rfObsCc-u",
    "outputId": "821f3a84-053c-4972-c10d-cf767dc40f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "CPU times: user 1min 55s, sys: 4min 27s, total: 6min 23s\n",
      "Wall time: 32min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_lr_smote = grid_Search(\"LR\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['LR'] = grid_lr_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCMXN9M8Cc-u"
   },
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NPGHPCruCc-v"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_per = grid_Search(\"PER\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "nZ1wmcmnCc-v"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_per_smote = grid_Search(\"PER\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['Perceptron'] = grid_per_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "\n",
    "La regression logistique n'est qu'un perceptron avec une sigmoid en fonction d'activation.\n",
    "On voit que la Regression Logistique a de meilleures performances à l'issue de la recherche d'hyperparamètres mais pas de loin. Par ailleurs le temps d'entrainement est considérablement plus élevé pour la regression logistique (du au calcul de l'exponentiel). Nous verrons par la suite quel modèle il est préférable de conserver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cy0mgWVCc-v"
   },
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9K-4Ufu3Cc-v"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_svc = grid_Search(\"SVC\",X_vect,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l4IjTNIiCc-w"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_svc_smote = grid_Search(\"SVC\",X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['SVC'] = grid_svc_smote.best_params_\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dico, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charging the best parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ct7BBebdCc-w",
    "outputId": "a7c78c52-cb68-4e11-9490-fb43ef8b6189"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "data_sgd = data['SGD']\n",
    "data_rf = data['RF']\n",
    "data_gb = data['GB']\n",
    "data_lr = data['LR']\n",
    "data_per = data['Perceptron']\n",
    "data_svc = data['SVC']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on all training data and testing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model,X_train_pred,y_train_pred):\n",
    "    predictions_train = model.predict(X_train_pred)\n",
    "    predictions_test = model.predict(X_vect_test)\n",
    "    accuracy_train = accuracy_score(y_train_pred,predictions_train )\n",
    "    accuracy_test = accuracy_score(y_test,predictions_test )\n",
    "    \n",
    "    print(f\"train_accuracy : {accuracy_train} \\ntest_accuracy : {accuracy_test}  \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FozzlotYCc-w",
    "outputId": "82bdacbd-ae58-49a0-faaf-c3ab0308b5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=5.357894736842105e-06, n_jobs=-1, penalty='l1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sgd_best = SGDClassifier()\n",
    "model_sgd_best.set_params(**data_sgd,n_jobs=-1) \n",
    "\n",
    "model_sgd_best_smote = SGDClassifier()\n",
    "model_sgd_best_smote.set_params(**data_sgd,n_jobs=-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 904 ms, total: 2.01 s\n",
      "Wall time: 382 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=5.357894736842105e-06, n_jobs=-1, penalty='l1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_sgd_best.fit(X_vect,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 882 ms, total: 2.24 s\n",
      "Wall time: 442 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=5.357894736842105e-06, n_jobs=-1, penalty='l1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_sgd_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9675632334718274 \n",
      "test_accuracy : 0.869141653501843  \n"
     ]
    }
   ],
   "source": [
    "testing(model_sgd_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9629280727030921 \n",
      "test_accuracy : 0.8686150605581885  \n"
     ]
    }
   ],
   "source": [
    "testing(model_sgd_best_smote,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "lGouwXZ_Cc-x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('rf_clf',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=300,\n",
       "                                        max_features='sqrt', min_samples_leaf=2,\n",
       "                                        min_samples_split=32, n_jobs=-1,\n",
       "                                        verbose=1, warm_start=True))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf_best = RandomForestClassifier()\n",
    "model_rf_best.set_params(**data_rf,n_jobs=-1)\n",
    "\n",
    "model_rf_best_smote = RandomForestClassifier()\n",
    "model_rf_best_smote.set_params(**data_rf,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "bdEu7AlyCc-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 669 ms, sys: 378 µs, total: 670 ms\n",
      "Wall time: 681 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliott/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:429: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('rf_clf',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=300,\n",
       "                                        max_features='sqrt', min_samples_leaf=2,\n",
       "                                        min_samples_split=32, n_jobs=-1,\n",
       "                                        verbose=1, warm_start=True))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_rf_best.fit(X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "yWtUmtDsCc-x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.8653934932089317 \n",
      "test_accuracy : 0.6908899420747762  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "testing(model_rf_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_rf_best_smote,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('gb_clf',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, n_estimators=800,\n",
       "                                            verbose=1))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gb_best = GradientBoostingClassifier()\n",
    "model_gb_best.set_params(**data_gb)\n",
    "\n",
    "model_gb_best_smote = GradientBoostingClassifier()\n",
    "model_gb_best_smote.set_params(**data_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 33s, sys: 9.32 ms, total: 9min 33s\n",
      "Wall time: 9min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('gb_clf',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2,\n",
       "                                            n_estimators=800))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_gb_best.fit(X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_gb_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9136963335520082 \n",
      "test_accuracy : 0.8156924697209057  \n"
     ]
    }
   ],
   "source": [
    "testing(model_gb_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gb_best_smote,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('lr_clf',\n",
       "                 LogisticRegression(C=10, max_iter=1000, n_jobs=-1, tol=1e-08,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_best = LogisticRegression(max_iter=1000)\n",
    "model_lr_best.set_params(**data_lr,n_jobs=-1)\n",
    "\n",
    "model_lr_best_smote = LogisticRegression(max_iter=1000)\n",
    "model_lr_best_smote.set_params(**data_lr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "CPU times: user 655 ms, sys: 293 ms, total: 949 ms\n",
      "Wall time: 14.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('lr_clf',\n",
       "                 LogisticRegression(C=10, max_iter=1000, n_jobs=-1, tol=1e-08,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr_best.fit(X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lr_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.959277887115193 \n",
      "test_accuracy : 0.8230647709320695  \n"
     ]
    }
   ],
   "source": [
    "testing(model_lr_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_lr_best_smote,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('per_clf',\n",
       "                 Perceptron(alpha=5.272631578947369e-06, n_jobs=-1,\n",
       "                            penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_per_best = Perceptron()\n",
    "model_per_best.set_params(**data_per,n_jobs=-1)\n",
    "\n",
    "model_per_best_smote = Perceptron()\n",
    "model_per_best_smote.set_params(**data_per,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 10 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.22 seconds.\n",
      "Convergence after 11 epochs took 0.22 seconds\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.28 seconds.\n",
      "Convergence after 15 epochs took 0.28 seconds\n",
      "CPU times: user 1.38 s, sys: 1.14 s, total: 2.52 s\n",
      "Wall time: 944 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('per_clf',\n",
       "                 Perceptron(alpha=5.272631578947369e-06, n_jobs=-1,\n",
       "                            penalty='l1', verbose=1))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_per_best.fit(X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_per_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.8576669825303107 \n",
      "test_accuracy : 0.7814639283833597  \n"
     ]
    }
   ],
   "source": [
    "testing(model_per_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_per_best_smote,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('svc_clf',\n",
       "                 LinearSVC(dual=False, max_iter=10000, penalty='l1',\n",
       "                           verbose=1))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc_best = LinearSVC(max_iter=10000)\n",
    "model_svc_best.set_params(**data_svc)\n",
    "\n",
    "model_svc_best_smote = LinearSVC(max_iter=10000)\n",
    "model_svc_best_smote.set_params(**data_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]...........*..........*....*\n",
      "optimization finished, #iter = 257\n",
      "Objective value = 13387.596363\n",
      "#nonzeros/#features = 4675/30795\n",
      ".............*...........*...*\n",
      "optimization finished, #iter = 271\n",
      "Objective value = 12866.620558\n",
      "#nonzeros/#features = 4813/30795\n",
      "...........*..........*...*.\n",
      "optimization finished, #iter = 250\n",
      "Objective value = 13900.984310\n",
      "CPU times: user 6.94 s, sys: 13.1 ms, total: 6.95 s\n",
      "Wall time: 6.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('svc_clf',\n",
       "                 LinearSVC(dual=False, max_iter=10000, penalty='l1',\n",
       "                           verbose=1))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nonzeros/#features = 4850/30795\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_svc_best.fit(X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_svc_best_smote.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9274728478752096 \n",
      "test_accuracy : 0.8496577145866245  \n"
     ]
    }
   ],
   "source": [
    "testing(model_svc_best,X_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_svc_best_smote,X_train_smote,y_train_smote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aSjG01FhCc-x"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "86JLNLKor1AL"
   },
   "outputs": [],
   "source": [
    "clf1 = model_sgd_best_smote\n",
    "\n",
    "clf2 = model_rf_best_smote\n",
    "\n",
    "clf3 = model_gb_best_smote\n",
    "\n",
    "clf4 = model_lr_best_smote\n",
    "\n",
    "clf5 = model_per_best_smote\n",
    "\n",
    "clf6 = model_svc_best_smote\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(\n",
    "     estimators=[('sgd', clf1), ('rf', clf2), ('gb', clf3), ('lr', clf4), ('per', clf5), ('svc', clf6)],\n",
    "     voting='hard')\n",
    "\n",
    "eclf2 = VotingClassifier(\n",
    "     estimators=[('sgd', clf1), ('lr', clf4), ('per', clf5), ('svc', clf6)],\n",
    "     voting='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 360.88, NNZs: 5102, Bias: 0.947182, T: 41157, Avg. loss: 0.486335\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 414.01, NNZs: 5851, Bias: -1.909098, T: 41157, Avg. loss: 0.605988\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 406.08, NNZs: 5633, Bias: -1.600516, T: 41157, Avg. loss: 0.579718\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 420.66, NNZs: 4001, Bias: -1.523806, T: 82314, Avg. loss: 0.268741\n",
      "Total training time: 0.05 seconds.-- Epoch 2\n",
      "Norm: 369.55, NNZs: 3544, Bias: 1.069173, T: 82314, Avg. loss: 0.234354\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 412.70, NNZs: 3927, Bias: -1.462631, T: 82314, Avg. loss: 0.262655\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 374.69, NNZs: 3254, Bias: 1.027108, T: 123471, Avg. loss: 0.214995\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 416.32, NNZs: 3547, Bias: -1.238832, T: 123471, Avg. loss: 0.238725\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 424.34, NNZs: 3624, Bias: -1.369941, T: 123471, Avg. loss: 0.243591\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 378.15, NNZs: 3109, Bias: 0.973131, T: 164628, Avg. loss: 0.207548\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 418.92, NNZs: 3333, Bias: -1.232200, T: 164628, Avg. loss: 0.228852\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 427.06, NNZs: 3418, Bias: -1.233322, T: 164628, Avg. loss: 0.234303\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 380.72, NNZs: 3029, Bias: 1.001786, T: 205785, Avg. loss: 0.202612\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 420.90, NNZs: 3186, Bias: -1.265533, T: 205785, Avg. loss: 0.223468\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 429.12, NNZs: 3284, Bias: -1.283154, T: 205785, Avg. loss: 0.228544\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 382.85, NNZs: 3000, Bias: 1.015469, T: 246942, Avg. loss: 0.199822\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.59, NNZs: 3110, Bias: -1.220478, T: 246942, Avg. loss: 0.220169\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 384.56, NNZs: 2974, Bias: 1.000921, T: 288099, Avg. loss: 0.197536\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 430.80, NNZs: 3210, Bias: -1.325734, T: 246942, Avg. loss: 0.225788\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 423.96, NNZs: 3046, Bias: -1.219762, T: 288099, Avg. loss: 0.217921\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 386.04, NNZs: 2962, Bias: 0.996184, T: 329256, Avg. loss: 0.195868\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 432.26, NNZs: 3142, Bias: -1.235629, T: 288099, Avg. loss: 0.223635\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 425.16, NNZs: 2991, Bias: -1.190760, T: 329256, Avg. loss: 0.216155\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 387.31, NNZs: 2953, Bias: 1.037875, T: 370413, Avg. loss: 0.194615\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 433.51, NNZs: 3103, Bias: -1.247245, T: 329256, Avg. loss: 0.222504\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 426.27, NNZs: 2960, Bias: -1.176976, T: 370413, Avg. loss: 0.215096\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 388.49, NNZs: 2934, Bias: 1.023350, T: 411570, Avg. loss: 0.193641\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 434.63, NNZs: 3079, Bias: -1.219030, T: 370413, Avg. loss: 0.221089\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 389.55, NNZs: 2945, Bias: 0.990808, T: 452727, Avg. loss: 0.192532\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 427.25, NNZs: 2940, Bias: -1.210376, T: 411570, Avg. loss: 0.214187\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 435.62, NNZs: 3041, Bias: -1.237477, T: 411570, Avg. loss: 0.219870\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 390.49, NNZs: 2918, Bias: 1.013324, T: 493884, Avg. loss: 0.191605\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 428.15, NNZs: 2933, Bias: -1.172813, T: 452727, Avg. loss: 0.212939\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 436.56, NNZs: 3039, Bias: -1.202731, T: 452727, Avg. loss: 0.219149\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 391.34, NNZs: 2898, Bias: 1.014393, T: 535041, Avg. loss: 0.191014\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 428.97, NNZs: 2896, Bias: -1.169951, T: 493884, Avg. loss: 0.212359\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 437.41, NNZs: 3018, Bias: -1.206500, T: 493884, Avg. loss: 0.218719\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 392.14, NNZs: 2893, Bias: 1.015973, T: 576198, Avg. loss: 0.190428\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 429.72, NNZs: 2897, Bias: -1.162087, T: 535041, Avg. loss: 0.211676\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 438.18, NNZs: 3009, Bias: -1.217034, T: 535041, Avg. loss: 0.218236\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 392.88, NNZs: 2902, Bias: 1.029818, T: 617355, Avg. loss: 0.189861\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 430.44, NNZs: 2876, Bias: -1.163043, T: 576198, Avg. loss: 0.211078\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 438.90, NNZs: 2991, Bias: -1.219193, T: 576198, Avg. loss: 0.217487\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 393.58, NNZs: 2887, Bias: 1.015620, T: 658512, Avg. loss: 0.189353\n",
      "Total training time: 0.30 seconds.\n",
      "Convergence after 16 epochs took 0.30 seconds\n",
      "Norm: 431.11, NNZs: 2870, Bias: -1.175340, T: 617355, Avg. loss: 0.210728\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 439.57, NNZs: 2982, Bias: -1.196030, T: 617355, Avg. loss: 0.217397\n",
      "Total training time: 0.30 seconds.\n",
      "Convergence after 15 epochs took 0.30 seconds\n",
      "Norm: 431.74, NNZs: 2858, Bias: -1.171222, T: 658512, Avg. loss: 0.210209\n",
      "Total training time: 0.31 seconds.\n",
      "Convergence after 16 epochs took 0.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0120            9.95m\n",
      "         2           0.9888            9.90m\n",
      "         3           0.9711            9.89m\n",
      "         4           0.9563            9.90m\n",
      "         5           0.9428            9.90m\n",
      "         6           0.9318            9.88m\n",
      "         7           0.9217            9.86m\n",
      "         8           0.9126            9.85m\n",
      "         9           0.9040            9.83m\n",
      "        10           0.8965            9.82m\n",
      "        20           0.8405            9.62m\n",
      "        30           0.8040            9.44m\n",
      "        40           0.7767            9.29m\n",
      "        50           0.7543            9.14m\n",
      "        60           0.7351            9.00m\n",
      "        70           0.7185            8.87m\n",
      "        80           0.7038            8.74m\n",
      "        90           0.6908            8.62m\n",
      "       100           0.6784            8.50m\n",
      "       200           0.5897            7.40m\n",
      "       300           0.5325            6.22m\n",
      "       400           0.4876            4.94m\n",
      "       500           0.4530            3.73m\n",
      "       600           0.4249            2.51m\n",
      "       700           0.4016            1.25m\n",
      "       800           0.3815            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 10 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.21 seconds.\n",
      "Convergence after 11 epochs took 0.21 seconds\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.26 seconds.\n",
      "Convergence after 15 epochs took 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]...........*..........*....**\n",
      "optimization finished, #iter = 254\n",
      "Objective value = 13387.596365\n",
      "#nonzeros/#features = 4668/30795\n",
      ".............*..........*...*\n",
      "optimization finished, #iter = 266\n",
      "Objective value = 12866.624502\n",
      "#nonzeros/#features = 4818/30795\n",
      "...........*..........*....*\n",
      "optimization finished, #iter = 251\n",
      "Objective value = 13900.999692\n",
      "#nonzeros/#features = 4853/30795\n",
      "CPU times: user 10min 8s, sys: 2.38 s, total: 10min 10s\n",
      "Wall time: 10min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('clf',\n",
       "                                               SGDClassifier(alpha=1.5873684210526315e-05,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             penalty='l1',\n",
       "                                                             verbose=1))])),\n",
       "                             ('rf',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('rf_clf',\n",
       "                                               RandomForestClassifier(criterion='entropy',\n",
       "                                                                      max_depth=300,\n",
       "                                                                      max_features='sqrt',\n",
       "                                                                      min_samples_leaf=2,\n",
       "                                                                      min_samples_split=32,\n",
       "                                                                      n_job...\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('lr_clf',\n",
       "                                               LogisticRegression(C=10,\n",
       "                                                                  max_iter=1000,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  tol=1e-08,\n",
       "                                                                  verbose=1))])),\n",
       "                             ('per',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('per_clf',\n",
       "                                               Perceptron(alpha=5.272631578947369e-06,\n",
       "                                                          n_jobs=-1,\n",
       "                                                          penalty='l1',\n",
       "                                                          verbose=1))])),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('svc_clf',\n",
       "                                               LinearSVC(dual=False,\n",
       "                                                         max_iter=10000,\n",
       "                                                         penalty='l1',\n",
       "                                                         verbose=1))]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eclf1.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.937944942537114 \n",
      "test_accuracy : 0.8499210110584519  \n"
     ]
    }
   ],
   "source": [
    "testing(eclf1,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 357.60, NNZs: 4998, Bias: 0.891696, T: 41157, Avg. loss: 0.484172\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 366.71, NNZs: 3474, Bias: 0.954372, T: 82314, Avg. loss: 0.235496\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 412.60, NNZs: 5878, Bias: -1.926124, T: 41157, Avg. loss: 0.598335\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 404.48, NNZs: 5595, Bias: -1.558111, T: 41157, Avg. loss: 0.571228\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 371.66, NNZs: 3172, Bias: 1.002502, T: 123471, Avg. loss: 0.215340\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 375.23, NNZs: 3087, Bias: 0.988975, T: 164628, Avg. loss: 0.207595\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 419.40, NNZs: 4059, Bias: -1.519952, T: 82314, Avg. loss: 0.269418\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 377.89, NNZs: 3030, Bias: 1.001752, T: 205785, Avg. loss: 0.202337\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 411.28, NNZs: 3882, Bias: -1.333694, T: 82314, Avg. loss: 0.263669\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 379.95, NNZs: 2992, Bias: 0.999959, T: 246942, Avg. loss: 0.199732\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.83, NNZs: 3610, Bias: -1.359196, T: 123471, Avg. loss: 0.246532\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 414.99, NNZs: 3507, Bias: -1.275707, T: 123471, Avg. loss: 0.239624\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 381.78, NNZs: 2972, Bias: 0.993897, T: 288099, Avg. loss: 0.197770\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 417.59, NNZs: 3304, Bias: -1.266331, T: 164628, Avg. loss: 0.229658\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 425.45, NNZs: 3418, Bias: -1.280198, T: 164628, Avg. loss: 0.235829\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 383.28, NNZs: 2943, Bias: 1.020195, T: 329256, Avg. loss: 0.195586\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 419.64, NNZs: 3183, Bias: -1.222767, T: 205785, Avg. loss: 0.224534\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 427.45, NNZs: 3300, Bias: -1.283721, T: 205785, Avg. loss: 0.231521\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 384.60, NNZs: 2918, Bias: 1.007534, T: 370413, Avg. loss: 0.194475\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 421.28, NNZs: 3135, Bias: -1.235039, T: 246942, Avg. loss: 0.220891\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 429.07, NNZs: 3226, Bias: -1.272840, T: 246942, Avg. loss: 0.228047\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 422.74, NNZs: 3091, Bias: -1.199238, T: 288099, Avg. loss: 0.218625\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 385.78, NNZs: 2897, Bias: 1.015107, T: 411570, Avg. loss: 0.193198\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 430.47, NNZs: 3151, Bias: -1.222827, T: 288099, Avg. loss: 0.225689\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 386.83, NNZs: 2881, Bias: 1.001824, T: 452727, Avg. loss: 0.192620Norm: 423.96, NNZs: 3052, Bias: -1.212330, T: 329256, Avg. loss: 0.216954\n",
      "Total training time: 0.18 seconds.\n",
      "\n",
      "-- Epoch 12Total training time: 0.18 seconds.\n",
      "\n",
      "-- Epoch 9Norm: 431.69, NNZs: 3089, Bias: -1.253840, T: 329256, Avg. loss: 0.224195\n",
      "\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 387.78, NNZs: 2882, Bias: 1.046557, T: 493884, Avg. loss: 0.191802\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 425.07, NNZs: 3021, Bias: -1.174385, T: 370413, Avg. loss: 0.215612\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 432.77, NNZs: 3044, Bias: -1.235774, T: 370413, Avg. loss: 0.222932\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 388.67, NNZs: 2878, Bias: 0.992886, T: 535041, Avg. loss: 0.191133\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 433.76, NNZs: 3039, Bias: -1.224990, T: 411570, Avg. loss: 0.221752\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 426.05, NNZs: 2980, Bias: -1.186296, T: 411570, Avg. loss: 0.214361\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 389.47, NNZs: 2867, Bias: 1.018017, T: 576198, Avg. loss: 0.190395\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 434.66, NNZs: 3002, Bias: -1.229990, T: 452727, Avg. loss: 0.220669\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 426.95, NNZs: 2966, Bias: -1.193738, T: 452727, Avg. loss: 0.213601\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 390.24, NNZs: 2861, Bias: 1.021295, T: 617355, Avg. loss: 0.189997\n",
      "Total training time: 0.25 seconds.\n",
      "Convergence after 15 epochs took 0.25 seconds\n",
      "Norm: 435.48, NNZs: 2985, Bias: -1.217901, T: 493884, Avg. loss: 0.220406\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 427.78, NNZs: 2963, Bias: -1.207875, T: 493884, Avg. loss: 0.212646\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 436.24, NNZs: 2967, Bias: -1.215438, T: 535041, Avg. loss: 0.219369\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 428.54, NNZs: 2954, Bias: -1.188396, T: 535041, Avg. loss: 0.212085\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 436.96, NNZs: 2944, Bias: -1.219945, T: 576198, Avg. loss: 0.219144\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 429.25, NNZs: 2954, Bias: -1.199237, T: 576198, Avg. loss: 0.211706\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 437.64, NNZs: 2950, Bias: -1.203350, T: 617355, Avg. loss: 0.218462\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 429.91, NNZs: 2951, Bias: -1.186284, T: 617355, Avg. loss: 0.211087\n",
      "Total training time: 0.31 seconds.\n",
      "Convergence after 15 epochs took 0.31 seconds\n",
      "Norm: 438.28, NNZs: 2938, Bias: -1.170180, T: 658512, Avg. loss: 0.218251\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 438.86, NNZs: 2926, Bias: -1.192893, T: 699669, Avg. loss: 0.217804\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 439.43, NNZs: 2920, Bias: -1.198078, T: 740826, Avg. loss: 0.217458\n",
      "Total training time: 0.36 seconds.\n",
      "Convergence after 18 epochs took 0.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        92385     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.52156D+04    |proj g|=  6.00600D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.72399D+04    |proj g|=  4.05238D+01\n",
      "\n",
      "At iterate  100    f=  1.38656D+04    |proj g|=  1.69239D+02\n",
      "\n",
      "At iterate  150    f=  1.31815D+04    |proj g|=  5.81206D+01\n",
      "\n",
      "At iterate  200    f=  1.29607D+04    |proj g|=  7.76878D+01\n",
      "\n",
      "At iterate  250    f=  1.28956D+04    |proj g|=  6.04622D+00\n",
      "\n",
      "At iterate  300    f=  1.28799D+04    |proj g|=  3.58044D+00\n",
      "\n",
      "At iterate  350    f=  1.28758D+04    |proj g|=  1.48195D+00\n",
      "\n",
      "At iterate  400    f=  1.28742D+04    |proj g|=  4.92087D-01\n",
      "\n",
      "At iterate  450    f=  1.28739D+04    |proj g|=  1.60575D+00\n",
      "\n",
      "At iterate  500    f=  1.28738D+04    |proj g|=  1.04958D+00\n",
      "\n",
      "At iterate  550    f=  1.28737D+04    |proj g|=  2.51328D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "92385    598    664      1     0     0   3.951D-01   1.287D+04\n",
      "  F =   12873.728360203968     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 59.23, NNZs: 10382, Bias: 0.150000, T: 41157, Avg. loss: 0.049036\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 67.12, NNZs: 12019, Bias: -0.280000, T: 41157, Avg. loss: 0.059772\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 65.96, NNZs: 11652, Bias: -0.330000, T: 41157, Avg. loss: 0.056884-- Epoch 2\n",
      "Norm: 79.38, NNZs: 8014, Bias: 0.140000, T: 82314, Avg. loss: 0.026856\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 88.65, NNZs: 8964, Bias: -0.300000, T: 82314, Avg. loss: 0.031492\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 96.96, NNZs: 5766, Bias: 0.150000, T: 123471, Avg. loss: 0.019641\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 112.52, NNZs: 4770, Bias: 0.130000, T: 164628, Avg. loss: 0.015376\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 106.18, NNZs: 6571, Bias: -0.270000, T: 123471, Avg. loss: 0.022868\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 126.18, NNZs: 4233, Bias: 0.100000, T: 205785, Avg. loss: 0.013265\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 86.97, NNZs: 8612, Bias: -0.360000, T: 82314, Avg. loss: 0.029703\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 121.38, NNZs: 5434, Bias: -0.320000, T: 164628, Avg. loss: 0.019034\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 139.33, NNZs: 3817, Bias: 0.070000, T: 246942, Avg. loss: 0.012474\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 104.62, NNZs: 6373, Bias: -0.300000, T: 123471, Avg. loss: 0.022644\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 151.42, NNZs: 3563, Bias: 0.070000, T: 288099, Avg. loss: 0.011298\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 135.76, NNZs: 4879, Bias: -0.200000, T: 205785, Avg. loss: 0.017038\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 163.03, NNZs: 3375, Bias: 0.090000, T: 329256, Avg. loss: 0.011255\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 119.69, NNZs: 5272, Bias: -0.290000, T: 164628, Avg. loss: 0.018423\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 149.12, NNZs: 4511, Bias: -0.200000, T: 246942, Avg. loss: 0.016190\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 173.93, NNZs: 3185, Bias: 0.050000, T: 370413, Avg. loss: 0.011008\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 133.43, NNZs: 4727, Bias: -0.160000, T: 205785, Avg. loss: 0.016190\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 161.66, NNZs: 4215, Bias: -0.240000, T: 288099, Avg. loss: 0.015398\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 184.34, NNZs: 3106, Bias: 0.060000, T: 411570, Avg. loss: 0.009966\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 146.51, NNZs: 4390, Bias: -0.180000, T: 246942, Avg. loss: 0.015128\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 173.52, NNZs: 4040, Bias: -0.180000, T: 329256, Avg. loss: 0.014580\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 194.19, NNZs: 2990, Bias: 0.070000, T: 452727, Avg. loss: 0.010174\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 158.66, NNZs: 4130, Bias: -0.190000, T: 288099, Avg. loss: 0.014525\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 184.63, NNZs: 3865, Bias: -0.180000, T: 370413, Avg. loss: 0.014548\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 203.72, NNZs: 2883, Bias: 0.060000, T: 493884, Avg. loss: 0.009653\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 170.05, NNZs: 3862, Bias: -0.160000, T: 329256, Avg. loss: 0.013677\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 194.99, NNZs: 3714, Bias: -0.190000, T: 411570, Avg. loss: 0.013899\n",
      "Total training time: 0.16 seconds.\n",
      "Convergence after 10 epochs took 0.16 seconds\n",
      "Norm: 212.83, NNZs: 2829, Bias: 0.020000, T: 535041, Avg. loss: 0.009958\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 180.87, NNZs: 3690, Bias: -0.140000, T: 370413, Avg. loss: 0.013417\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 221.62, NNZs: 2798, Bias: 0.030000, T: 576198, Avg. loss: 0.009483\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.22, NNZs: 2746, Bias: 0.070000, T: 617355, Avg. loss: 0.009275\n",
      "Total training time: 0.19 seconds.\n",
      "Convergence after 15 epochs took 0.19 seconds\n",
      "Norm: 191.28, NNZs: 3570, Bias: -0.170000, T: 411570, Avg. loss: 0.013108\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 201.48, NNZs: 3511, Bias: -0.090000, T: 452727, Avg. loss: 0.013572\n",
      "Total training time: 0.18 seconds.\n",
      "Convergence after 11 epochs took 0.18 seconds\n",
      "[LibLinear]...........*..........*....*\n",
      "optimization finished, #iter = 251\n",
      "Objective value = 13387.596367\n",
      "#nonzeros/#features = 4672/30795\n",
      "............*...........*....*\n",
      "optimization finished, #iter = 272\n",
      "Objective value = 12866.624466\n",
      "#nonzeros/#features = 4817/30795\n",
      "...........*..........*...*.\n",
      "optimization finished, #iter = 250\n",
      "Objective value = 13900.986476\n",
      "#nonzeros/#features = 4854/30795\n",
      "CPU times: user 9.47 s, sys: 1.77 s, total: 11.2 s\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('clf',\n",
       "                                               SGDClassifier(alpha=1.5873684210526315e-05,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             penalty='l1',\n",
       "                                                             verbose=1))])),\n",
       "                             ('lr',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('lr_clf',\n",
       "                                               LogisticRegression(C=10,\n",
       "                                                                  max_iter=1000,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  tol=1e-08,\n",
       "                                                                  verbose=1))])),\n",
       "                             ('per',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('per_clf',\n",
       "                                               Perceptron(alpha=5.272631578947369e-06,\n",
       "                                                          n_jobs=-1,\n",
       "                                                          penalty='l1',\n",
       "                                                          verbose=1))])),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                              ('svc_clf',\n",
       "                                               LinearSVC(dual=False,\n",
       "                                                         max_iter=10000,\n",
       "                                                         penalty='l1',\n",
       "                                                         verbose=1))]))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eclf2.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy : 0.9355881138081007 \n",
      "test_accuracy : 0.8522906793048973  \n"
     ]
    }
   ],
   "source": [
    "testing(eclf2,X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_perso = [\"I think covid is the most horrible threat we ever faced\",\n",
    "                \"I love covid, thanks to it I can see my family much more often and I don't have to comute as much\",\n",
    "                \"I would love to come to your birthday party, but I got covid, I have to stay confined\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perso_vect = vectorizer.transform(tests_perso)\n",
    "predictions_out = eclf2.predict(test_perso_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main_notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "822dd2d5dd360abba25557036e653898358cd5c8dbb7b3b755070434497e6489"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
